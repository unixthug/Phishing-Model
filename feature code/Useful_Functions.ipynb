{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ef6e11-3da0-44bb-a0ed-1e75af2f510f",
   "metadata": {},
   "source": [
    "# WHOIS Domain Age Checker\n",
    "\n",
    "This code needs to be check and cited: https://github.com/stephensheridan/python-domain-age/blob/master/domain_age.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55585743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Stephen Sheridan\n",
    "# Date: 14/03/2019\n",
    "\n",
    "# Whois example output : as can be seen whois output format is not standardised\n",
    "\n",
    "# Domain:                  itb.ie\n",
    "# Registration Date:       11-January-1999\n",
    "# Domain Name: yahoo.com\n",
    "# Creation Date: 1995-01-18T00:00:00-0800\n",
    "# domain:        BIT-EXCHANGER.RU\n",
    "# created:       2016-07-04T15:30:24Z\n",
    "# Domain name:\n",
    "#         naturesaid.co.uk\n",
    "#     Relevant dates:\n",
    "#         Registered on: 25-Oct-1999\n",
    "\n",
    "# NOTE: cannot grep with word \"created\" as it appears in more than one place in whois info\n",
    "\n",
    "import subprocess\n",
    "from datetime  import *\n",
    "from domain_tests import *\n",
    "import dateutil.parser as p\n",
    "\n",
    "#url1 = \"itb.ie\"\n",
    "url1 = \"night-fever.it\"\n",
    "\n",
    "def getDaysAlive(url):\n",
    "    \"\"\"\n",
    "    Uses whois and grep to return number of days a domain has been alive based\n",
    "    on the created/registered date.\n",
    "\n",
    "    Parameters:\n",
    "    argument1 (String): domain to check\n",
    "\n",
    "    Returns:\n",
    "    int: -1 No match from whois - domain does not exist\n",
    "         -2 whois didn't return what we expected - no reg date\n",
    "         -3 there was a problem parsing a valid date from the whois data\n",
    "\n",
    "    Note1: This function is dependant on whois and grep being available form the command line\n",
    "    Note2: This function is also dependant on the dateutil library: pip install py-dateutil\n",
    "   \"\"\"\n",
    "    # Use grep to strip out the part of the output that we need\n",
    "    grep_filter = \" | grep -E \\\"Registration Date|Registered on|Creation Date|created:|Created:|Registration Time:|No match for domain\\\"\"\n",
    "\n",
    "    # Call the whois and pipe the output to grep\n",
    "    whois_data = subprocess.Popen(\"whois \" + url + grep_filter, shell=True, stdout=subprocess.PIPE).stdout.read()\n",
    "\n",
    "    # whois could not find a match for the domain - doesn't exist\n",
    "    if (\"No match for domain\" in whois_data):\n",
    "        return -1\n",
    "\n",
    "    # Split the output based on carriage returns (each line of output from grep)\n",
    "    whois_data = whois_data.strip().split('\\n')\n",
    "\n",
    "    # Only one date entry found - should be two ?? (Server followed by creation date of domain)\n",
    "    if (len(whois_data) == 1):\n",
    "        return -2\n",
    "\n",
    "    # Try to parse a datetime object out of the string\n",
    "    # NOTE: we are assuming that the last entry in the list returned from whois and grep\n",
    "    # will be the actual registration/creation data of the domain in question: whois_data[-1]\n",
    "    try:\n",
    "        # Fingers crossed we get a valid date out of the string\n",
    "        reg_date =  p.parse(whois_data[-1].lower(), fuzzy=True)\n",
    "    except:\n",
    "        return -3\n",
    "\n",
    "    # Get datetime stamp based on NOW!\n",
    "    today = datetime.today()\n",
    "    # Timezone and no timezones can cause problems when comparing\n",
    "    # Strip timezone info from each datetime object (not a very good idea - fudge!!)\n",
    "    today = today.replace(tzinfo=None)\n",
    "    reg_date = reg_date.replace(tzinfo=None)\n",
    "    # Return the days alive (Diff between dates)\n",
    "    return today - reg_date\n",
    "\n",
    "\n",
    "# Test the function with a list of domain names ...........\n",
    "failed = []\n",
    "for domain in test_domains:\n",
    "    days_alive = getDaysAlive(domain)\n",
    "    if (days_alive == -1):\n",
    "        failed.append(domain)\n",
    "    print domain + \" days alive = \" + str(getDaysAlive(domain))\n",
    "\n",
    "print \"No. of domain names tested: \" + str(len(test_domains))\n",
    "print \"No. of failures: \" + len(failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2d5b3-3248-4345-9177-c26411ce4245",
   "metadata": {},
   "source": [
    "# HTTP Secuirity Header Checker \n",
    "\n",
    "Still need to be developed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc42d6d-4b5e-4a39-aad9-dc5db717b153",
   "metadata": {},
   "source": [
    "# Basic Website security tool\n",
    "\n",
    "Based on this website: https://www.freecodecamp.org/news/build-a-web-application-security-scanner-with-python/\n",
    "\n",
    "Sql Injection, Cross -site scripting, sensitive information exposure, basic authentication weakness \n",
    "\n",
    "This code doesnt work in a jupyter environmwent, needs to be converted into python and experimented with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1566c94e-7efd-475b-bf10-30fc1fba5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\josiah\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.4.6)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\josiah\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.7/64.7 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.7/107.7 kB ? eta 0:00:00\n",
      "Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "   ---------------------------------------- 0.0/131.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 131.6/131.6 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.9/152.9 kB ? eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.4-cp311-cp311-win_amd64.whl (106 kB)\n",
      "   ---------------------------------------- 0.0/107.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.0/107.0 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.0/71.0 kB ? eta 0:00:00\n",
      "Downloading soupsieve-2.8.3-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: urllib3, soupsieve, idna, charset_normalizer, certifi, requests, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.14.3 certifi-2026.1.4 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 soupsieve-2.8.3 urllib3-2.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: C:\\Users\\Josiah\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 urllib3 colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af8778f-7c19-422d-8bfb-b9c7e5d77e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required packages \n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import urllib.parse \n",
    "import colorama \n",
    "import re \n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "import sys \n",
    "from typing import List, Dict, Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36dcb1c7-acc2-416e-bf10-9349e27c93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building out the bread and butter class \n",
    "class WebSecurityScanner:\n",
    "    def __init__(self, target_url: str, max_depth: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the security scanner with a target URL and maximum crawl depth.\n",
    "\n",
    "        Args:\n",
    "            target_url: The base URL to scan\n",
    "            max_depth: Maximum depth for crawling links (default: 3)\n",
    "        \"\"\"\n",
    "        self.target_url = target_url\n",
    "        self.max_depth = max_depth\n",
    "        self.visited_urls: Set[str] = set()\n",
    "        self.vulnerabilities: List[Dict] = []\n",
    "        self.session = requests.Session()\n",
    "\n",
    "        # Initialize colorama for cross-platform colored output\n",
    "        colorama.init()\n",
    "\n",
    "    def normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Normalize the URL to prevent duplicate checks\"\"\"\n",
    "        parsed = urllib.parse.urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "\n",
    "    # checking content on the page \n",
    "    def crawl(self, url: str, depth: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Crawl the website to discover pages and endpoints.\n",
    "\n",
    "        Args:\n",
    "            url: Current URL to crawl\n",
    "            depth: Current depth in the crawl tree\n",
    "        \"\"\"\n",
    "        if depth > self.max_depth or url in self.visited_urls:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.visited_urls.add(url)\n",
    "            response = self.session.get(url, verify=False)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all links in the page\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                next_url = urllib.parse.urljoin(url, link['href'])\n",
    "                if next_url.startswith(self.target_url):\n",
    "                    self.crawl(next_url, depth + 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling {url}: {str(e)}\")\n",
    "\n",
    "    def check_sql_injection(self, url: str) -> None:\n",
    "        \"\"\"Test for potential SQL injection vulnerabilities\"\"\"\n",
    "        sql_payloads = [\"'\", \"1' OR '1'='1\", \"' OR 1=1--\", \"' UNION SELECT NULL--\"]\n",
    "\n",
    "        for payload in sql_payloads:\n",
    "            try:\n",
    "                # Test GET parameters\n",
    "                parsed = urllib.parse.urlparse(url)\n",
    "                params = urllib.parse.parse_qs(parsed.query)\n",
    "\n",
    "                for param in params:\n",
    "                    test_url = url.replace(\n",
    "                        f\"{param}={params[param][0]}\",\n",
    "                        f\"{param}={payload}\"\n",
    "                    )\n",
    "                    response = self.session.get(test_url)\n",
    "\n",
    "                    # Look for SQL error messages\n",
    "                    if any(error in response.text.lower() for error in\n",
    "                           ['sql', 'mysql', 'sqlite', 'postgresql', 'oracle']):\n",
    "                        self.report_vulnerability({\n",
    "                            'type': 'SQL Injection',\n",
    "                            'url': url,\n",
    "                            'parameter': param,\n",
    "                            'payload': payload\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error testing SQL injection on {url}: {str(e)}\")\n",
    "\n",
    "    def check_xss(self, url: str) -> None:\n",
    "        \"\"\"Test for potential Cross-Site Scripting vulnerabilities\"\"\"\n",
    "        xss_payloads = [\n",
    "            \"<script>alert('XSS')</script>\",\n",
    "            \"<img src=x onerror=alert('XSS')>\",\n",
    "            \"javascript:alert('XSS')\"\n",
    "        ]\n",
    "\n",
    "        for payload in xss_payloads:\n",
    "            try:\n",
    "                # Test GET parameters\n",
    "                parsed = urllib.parse.urlparse(url)\n",
    "                params = urllib.parse.parse_qs(parsed.query)\n",
    "\n",
    "                for param in params:\n",
    "                    test_url = url.replace(\n",
    "                        f\"{param}={params[param][0]}\",\n",
    "                        f\"{param}={urllib.parse.quote(payload)}\"\n",
    "                    )\n",
    "                    response = self.session.get(test_url)\n",
    "\n",
    "                    if payload in response.text:\n",
    "                        self.report_vulnerability({\n",
    "                            'type': 'Cross-Site Scripting (XSS)',\n",
    "                            'url': url,\n",
    "                            'parameter': param,\n",
    "                            'payload': payload\n",
    "                        })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error testing XSS on {url}: {str(e)}\")\n",
    "\n",
    "    def check_sensitive_info(self, url: str) -> None:\n",
    "        \"\"\"Check for exposed sensitive information\"\"\"\n",
    "        sensitive_patterns = {\n",
    "            'email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "            'phone': r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "            'ssn': r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n",
    "            'api_key': r'api[_-]?key[_-]?([\\'\"|`])([a-zA-Z0-9]{32,45})\\1'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "\n",
    "            for info_type, pattern in sensitive_patterns.items():\n",
    "                matches = re.finditer(pattern, response.text)\n",
    "                for match in matches:\n",
    "                    self.report_vulnerability({\n",
    "                        'type': 'Sensitive Information Exposure',\n",
    "                        'url': url,\n",
    "                        'info_type': info_type,\n",
    "                        'pattern': pattern\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking sensitive information on {url}: {str(e)}\")\n",
    "\n",
    "    def scan(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Main scanning method that coordinates the security checks\n",
    "\n",
    "        Returns:\n",
    "            List of discovered vulnerabilities\n",
    "        \"\"\"\n",
    "        print(\n",
    "            f\"\\n{colorama.Fore.BLUE}Starting security scan of \"\n",
    "            f\"{self.target_url}{colorama.Style.RESET_ALL}\\n\"\n",
    "        )\n",
    "\n",
    "        # First, crawl the website\n",
    "        self.crawl(self.target_url)\n",
    "\n",
    "        # Then run security checks on all discovered URLs\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for url in self.visited_urls:\n",
    "                executor.submit(self.check_sql_injection, url)\n",
    "                \n",
    "                executor.submit(self.check_xss, url)\n",
    "                executor.submit(self.check_sensitive_info, url)\n",
    "\n",
    "        return self.vulnerabilities\n",
    "\n",
    "    def report_vulnerability(self, vulnerability: Dict) -> None:\n",
    "        \"\"\"Record and display found vulnerabilities\"\"\"\n",
    "        self.vulnerabilities.append(vulnerability)\n",
    "        print(f\"{colorama.Fore.RED}[VULNERABILITY FOUND]{colorama.Style.RESET_ALL}\")\n",
    "        for key, value in vulnerability.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    target_url = input(\"Enter the target URL (e.g., https://example.com): \").strip()\n",
    "\n",
    "    if not target_url:\n",
    "        print(\"Error: No URL provided.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    scanner = WebSecurityScanner(target_url)\n",
    "    vulnerabilities = scanner.scan()\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{colorama.Fore.GREEN}Scan Complete!{colorama.Style.RESET_ALL}\")\n",
    "    print(f\"Total URLs scanned: {len(scanner.visited_urls)}\")\n",
    "    print(f\"Vulnerabilities found: {len(vulnerabilities)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74774b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
